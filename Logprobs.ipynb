{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSFVxHN/GyobwgFvBUBFvX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaoshiang/Gaussian-Mixture-Models-via-Backprop-and-Expectation-Maximization/blob/main/Logprobs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logprobs\n",
        "\n",
        "AKA: fun and games with log, exp, softmax, crossentropy, and negative log loss.ipynb\n",
        "\n",
        "If you are familiar with logits and probability distributions, but curious about log probs, this notebook is for you.\n",
        "\n",
        "## Why do we need to understand log-probs?\n",
        "\n",
        "Many LLM APIs now expose log-probs of the tokens it generates. This is a high resolution estimate of confidence of the model.\n",
        "\n",
        "Most importantly, **you can use these probabilities** to estimate the probability or confidence of a model when trying to judge its calibration.\n",
        "\n",
        "For example, this is the official Open AI cookbook for working with log probs:\n",
        "\n",
        "https://cookbook.openai.com/examples/using_logprobs\n",
        "\n",
        "## The basics: sigmoid and softmax.\n",
        "\n",
        "Before we get into log-probs, let's cover some basics.\n",
        "\n",
        "Why do we use sigmoid and softmax as activation functions for classifiers?\n",
        "\n",
        "Becuase they turn the output of a neural network model, which are in the range (-inf, inf), into the range (0, 1). And we can interpret numbers in the range (0, 1) as percentages or probabilities.\n",
        "\n",
        "The outputs of a neural network before applying sigmoid or softmax are called logits."
      ],
      "metadata": {
        "id": "VsDzEA7qb1ZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid\n",
        "\n",
        "Let's make sure we can believe the statement that sigmoid turns logits, or numbers in the range (-inf, inf), into numbers in the range (0, 1). Let's pick some corner case numbers, run sigmoid, and see if the output is indeed in the range (0,1)."
      ],
      "metadata": {
        "id": "CYgfUESgSMvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Make the output human readable.\n",
        "torch.set_printoptions(precision=2, sci_mode=False)"
      ],
      "metadata": {
        "id": "kPmFw0PVSEoI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corner_case_logits = torch.Tensor([-5.0, -1.0, 0.0, 1.0, 5])\n",
        "outputs = F.sigmoid(corner_case_logits)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEpTiVLaSev_",
        "outputId": "bd6ccbc0-57d4-44b3-b232-3f0c154b2ec4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.01, 0.27, 0.50, 0.73, 0.99])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look good! The logits do indeed get squished into the range (0, 1). This means that for the numbers that we get out of a neural network, we can run the sigmoid function and interpret it as a 1% chance, 27% chance, 50% chance, 73% chance, and 99% chance of being a positive."
      ],
      "metadata": {
        "id": "sFOKr15_SrEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax\n",
        "\n",
        "Softmax takes a set of logits and like sigmoid, squishes them into the range (0,1). It has another important attribute: the sum of the numbers will be 1.0, aka 100%. Let's test that empirically.\n",
        "\n",
        "Suppose the output of a model classifying dogs, cats, and birds is 2.0, 1.0, and -1.0. Those clearly don't sum to one. And one of the values is negative! This is definitely not a probability distribution.\n",
        "\n",
        "But after running softmax, we do get a probably distribution: each value is between zero and one, and the sum of 71%, 26%, and 4% is indeed 100%."
      ],
      "metadata": {
        "id": "ucc7cf8ccGQ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCP3hMwVbtyJ",
        "outputId": "0e226660-f87d-453a-974f-57aac587995d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a probability distribution and it should sum to one: tensor([0.71, 0.26, 0.04])\n"
          ]
        }
      ],
      "source": [
        "logits = torch.Tensor([2.0, 1.0, -1.0])\n",
        "\n",
        "probability_distribution = F.softmax(logits, dim=-1)\n",
        "print(\"This is a probability distribution and it should sum to one:\", probability_distribution)\n",
        "\n",
        "assert 0.99 < sum(probability_distribution.tolist()) < 1.01"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax is invariant to scalar shifts.\n",
        "\n",
        "Now let's see what happens if we shift these logits by a constant amount. Let's add 10 to all the logits."
      ],
      "metadata": {
        "id": "DBS1TdFaVY0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shifted_logits = torch.Tensor([12.0, 11.0, 9.0])\n",
        "probability_distribution = F.softmax(shifted_logits, dim=-1)\n",
        "print(\"This is a probability distribution and it should sum to one:\", probability_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc1ILi2FVf9p",
        "outputId": "d137b1c5-d5d7-4016-bac6-7d5441ef65a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a probability distribution and it should sum to one: tensor([0.71, 0.26, 0.04])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting! The output is the same! The fancy way of saying this is that the softmax function is invariant to constant shifts in the input logits."
      ],
      "metadata": {
        "id": "38JkoL9NVuDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numerical Instability\n",
        "\n",
        "However, just because softmax is mathematically invariant to constant shifts, doesn't mean we can go crazy when actually doing math on silicon. A 32 bit floating point number has about 7 significant digits (ask ChatGPT about mantissa and exponent if you wanna learn more).\n",
        "\n",
        "So if we add big number, like 10,000,000, the computer will round away the 2, 1 and -1 and just see 10,000,000. And running softmax of a vector of three numbers, all of which are the same, will result in a uniform distribution: 33.3%, 33.3%, 33.3%."
      ],
      "metadata": {
        "id": "R13lktwTWhGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shifted_logits = logits + 1000000000000000\n",
        "probability_distribution = F.softmax(shifted_logits, dim=-1)\n",
        "print(\"This is not the probability distribution we wanted.\", probability_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrab8c6qWSY3",
        "outputId": "f9881565-2153-477d-f2b5-37099c786359"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is not the probability distribution we wanted. tensor([0.33, 0.33, 0.33])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem actually gets worse than that. When training a neural network, we run an algorithm called back propagation, which requires that we calculate gradients of a function. Gradients are the first-order derivative - the slope of a line. If the line is really flat, your gradients will round down to zero and your model won't train. If the line is really vertical, your gradients will approach infinity.\n",
        "\n",
        "Unfortunately, the softmax function uses exponents, and the loss function of a model uses logs. Both of these functions can get very vertical and very flat at the extremes. This means that for the shifted logits, even if they should give the same numbers after running softmax, due to numerical instability, you'll instead get a NaN or inf.\n",
        "\n",
        "Let's run a simple backprop algorithm in pytorch to prove this.\n",
        "\n",
        "Notice that the gradients from the big logits (102 and 101) are NaN, or Not A Number. This means the values overflowed, which would ruin any neural network training."
      ],
      "metadata": {
        "id": "IjFU2JpyXw2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_backprop(logits):\n",
        "  # Run softmax.\n",
        "  exp_logits = torch.exp(logits)\n",
        "  sum_exp_logits = torch.sum(exp_logits)\n",
        "  prediction = exp_logits / sum_exp_logits\n",
        "\n",
        "  loss = torch.log(prediction).mean()\n",
        "  loss.backward()\n",
        "  return prediction, logits.grad\n",
        "\n",
        "# Create a small input value\n",
        "input = torch.tensor([2.0, 1.0], requires_grad=True)\n",
        "shifted_input = torch.tensor([102.0, 101.0], requires_grad=True)\n",
        "\n",
        "# Print the gradients\n",
        "print(f\"Gradient for: {input.tolist()}: \", simple_backprop(input)[1])\n",
        "print(f\"Gradient for: {shifted_input.tolist()}: \", *simple_backprop(shifted_input)[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvtq30q62rUx",
        "outputId": "c3aeee7c-34ca-4997-9353-19aceec327a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient for: [2.0, 1.0]:  tensor([-0.23,  0.23])\n",
            "Gradient for: [102.0, 101.0]:  tensor(nan) tensor(nan)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diving into the math to find the numerical stable log-softmax function.\n",
        "\n",
        "We are going to have to dig into the greek letters to figure out how to stabilize this math.\n",
        "\n",
        "Softmax is defined as:\n",
        "\n",
        "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
        "\n",
        "Where $z_i$ is the logit for class $i$.\n",
        "\n",
        "Notice all those exponents... running $e^{101}$ is what caused our previous code to overflow into NaNs.\n",
        "\n",
        "Crossentropy is defined as:\n",
        "\n",
        "$$CE = -\\sum_{i=1}^K y_i \\log(p_i)$$\n",
        "\n",
        "Where $y_i$ is really just a switch that turns on a specific $p_i$ for the true label (e.g. dog), $p_i$ is the output of the softmax function.\n",
        "\n",
        "Combining the two equations so we can calculate crossentropy direction on a logit yields:\n",
        "\n",
        "$$CE = -\\sum_{i=1}^K y_i \\log\\left(\\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\\right)$$\n",
        "\n",
        "We know that the $\\log(\\frac{a}{b}) = \\log(a) - \\log(b)$, and $\\log(e^x) = x$ so we can adjust the above to\n",
        "\n",
        "$$ CE = -\\sum_{i=1}^K y_i \\left(z_i - \\log\\left(\\sum_{j=1}^K e^{z_j}\\right)\\right)$$\n",
        "\n",
        "Let's look deeper inside the key of that equation: it appears to be just a constant shift of all the logits, $z_1, z_2, z_3, ..., z_k$. In fact, that equation is the log-softmax of the logits.\n",
        "\n",
        "$$ \\text{logsoftmax}(z_j) = z_i - \\log\\sum_{j=1}^K e^{z_j}$$\n",
        "\n",
        "We have freed the logit itself, but, we haven't solved the calculation of the right hand side of the equation... we still see the log of the sum of the exponent of a series of numbers, so it appears we are still stuck taking the exponent of big numbers.\n",
        "\n",
        "Luckily, we have the log-sum-exp trick to stabilize that part of the equation.\n"
      ],
      "metadata": {
        "id": "8LfVIXVDK1Zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log-Sum-Exp\n",
        "\n",
        "Anytime you are taking the log of the sum of the exponents, know that you can factor out some of the big numbers (e.g $e^{101}$) using the log-sum-exp technique.\n",
        "\n",
        "First, we can immediately subtract a big number $M$ from all the logits, then multiply the resulting value by $e^M$.\n",
        "\n",
        "$$\\log \\left( \\sum_{i=1}^n e^{z_i} \\right) = \\log \\left( e^M \\sum_{i=1}^n e^{z_i - M} \\right)$$\n",
        "\n",
        "Then, we find the log of a product of two values, and can move that to be the sum of two logs.\n",
        "\n",
        "$$ = \\log(e^M) + \\log \\left( \\sum_{i=1}^n e^{x_i - M} \\right)$$\n",
        "\n",
        "And we know the log of an exponent of a value is just the value...\n",
        "\n",
        "$$ = M + \\log \\left( \\sum_{i=1}^n e^{x_i - M} \\right)$$\n",
        "\n",
        "so we have effectively reduced the scale of our logits while adding just a constant value to the log-sum-exp of the reduced logits. Voila! Numerical stability."
      ],
      "metadata": {
        "id": "NVFHNUhLmAcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nice Properties of Log-Probs\n",
        "\n",
        "The log prob output by APIs like Open AI are really just the logits shifted by a constant amount so that they have some nicer properties.\n",
        "\n",
        "Since probabilities are always between zero and one, the log-prob will always be less than zero. A high confidence prediction would have a prob of say 0.999, yielding a log-prob of something like -1e-3. A low confidence prediction would have a log prob that looks something like -0.5.\n",
        "\n",
        "If you want to calculate the actual probability of a specific token, just take the exponent of the log-prob. No need for worrying about the other [30,521] possible tokens that the model output to calculate a sum - a single log-prob for a single token is all you need to know the probability of that token.\n",
        "\n",
        "Log probs always can be flowed into a loss function almost directly, without need for logs or exponents or crossentropy or softmax. The loss function for this in pytorch is\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
        "\n"
      ],
      "metadata": {
        "id": "Wy9Sm9KQbvRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing log probs"
      ],
      "metadata": {
        "id": "nqzSWOtifYt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to prove some of the math above. Let's set up some logits, calculate the log-softmax the hard way. We see that indeed, all the values are negative."
      ],
      "metadata": {
        "id": "oRJzaw1Hb37H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor([102.0, 101.0])\n",
        "log_probs = F.log_softmax(logits, dim=-1)\n",
        "print(log_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stOoZ1nlb4x2",
        "outputId": "29ccf3cf-ef58-4347-ad76-dbe2221ee61c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.31, -1.31])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see if the logits and the log-probs output the same probability distribution. Yup, that checks out too!"
      ],
      "metadata": {
        "id": "tHEiZ4kMduxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(F.softmax(logits, dim=-1))\n",
        "print(F.softmax(log_probs, dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VxCbrpAd54M",
        "outputId": "b62ae6ad-2025-4432-a99c-679ea239fe66"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.73, 0.27])\n",
            "tensor([0.73, 0.27])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's prove that we can calculate the probabilities simply by running the exp function, not the softmax, when we have the log_probs. Once again, things look good.\n"
      ],
      "metadata": {
        "id": "i7UaBrrKeOZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.exp(log_probs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y-_s10deNH7",
        "outputId": "7d9b29cf-d1e5-403f-8fc6-dc236238bf5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.73, 0.27])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's make sure that this is indeed numerically stable. Can we get good gradients with big numbers? Recall we overflowed previously."
      ],
      "metadata": {
        "id": "zDzj93IEef0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shifted_input = torch.tensor([102.0, 101.0], requires_grad=True)\n",
        "log_probs = F.log_softmax(shifted_input, dim=-1)\n",
        "log_probs.retain_grad()\n",
        "print(f\"Gradients for: {shifted_input.tolist()}: \", simple_backprop(log_probs)[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEJHBhA8eeQA",
        "outputId": "a8a8d1b9-b86a-4490-b77e-0761b2a3e786"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients for: [102.0, 101.0]:  tensor([-0.23,  0.23])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result are indeed good gradients!"
      ],
      "metadata": {
        "id": "4Ufw9b0jhJ3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced loss functions in TensorFlow\n",
        "\n",
        "If you've been programming in Tensorflow, you might be a little worried right now - you've been activating your neural networks with Softmax and then using the categorical_crossentropy loss function, just like you were taught in that basic ML course. Were you risking numerical instability?\n",
        "\n",
        "TF/Keras ignores what you implemented and operates directly on the logits. See this code block:\n",
        "\n",
        "https://github.com/tensorflow/tensorflow/blob/e7e2b655d573de99c7c7fd2ceb5659f57fb2d908/tensorflow/python/keras/backend.py#L4762\n",
        "\n",
        "```python\n",
        "  # Use logits whenever they are available. `softmax` and `sigmoid`\n",
        "  # activations cache logits on the `output` Tensor.\n",
        "  if hasattr(output, '_keras_logits'):\n",
        "    output = output._keras_logits  # pylint: disable=protected-access\n",
        "    if from_logits:\n",
        "      warnings.warn(\n",
        "          '\"`categorical_crossentropy` received `from_logits=True`, but '\n",
        "          'the `output` argument was produced by a sigmoid or softmax '\n",
        "          'activation and thus does not represent logits. Was this intended?\"')\n",
        "    from_logits = True\n",
        "```\n",
        "\n",
        "Then, when it's operating on the logits, it uses the log_softmax function to apply the constant shift of the logits by the numerically stable implementation of log-sum-exp.\n",
        "\n",
        "https://github.com/tensorflow/tensorflow/blob/e7e2b655d573de99c7c7fd2ceb5659f57fb2d908/tensorflow/python/ops/nn_ops.py#L4149C8-L4149C49\n",
        "\n",
        "```python\n",
        "    # Do the actual op computation.\n",
        "    if config.is_op_determinism_enabled():\n",
        "      log_probs = log_softmax_v2(precise_logits)\n",
        "      cost = -math_ops.reduce_sum(labels * log_probs, axis=1)\n",
        "```"
      ],
      "metadata": {
        "id": "wt0_x1MBbnn8"
      }
    }
  ]
}